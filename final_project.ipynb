{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "### Import helper functions\n",
    "from helper_functions import plotting_salary_expenses, dataset_info,\\\n",
    "    remove_outlier, ratio\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary',\n",
    "                'bonus',\n",
    "                'long_term_incentive',\n",
    "                'deferred_income',\n",
    "                'deferral_payments',\n",
    "                'loan_advances',\n",
    "                'other',\n",
    "                'expenses',\n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "#my_dataset = data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting_salary_expenses(data_dict, 'salary', 'expenses')\n",
    "#plotting_salary_expenses(data_dict, 'salary', 'from_poi_to_this_person')\n",
    "#plotting_salary_expenses(data_dict, 'from_poi_to_this_person', 'from_this_person_to_poi')\n",
    "#plotting_salary_expenses(data_dict, 'salary', 'from_this_person_to_poi')\n",
    "#plotting_salary_expenses(data_dict, 'shared_receipt_with_poi', 'to_messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to figure out the persons of interest using machine learning. Machine learning was usefull here because we can put many features such as salary, emails to poi, bonuses and predict if the person is POI or not based on these features. I got hold of excerpt of the data from enron corpus, and tried to create a model to predict persons of interest. Exploring the data set I got these statistics:\n",
    "\n",
    "* Number of total datapoints:  146\n",
    "* Number of features for each datapoint:  21\n",
    "* Number of persons of interest in this dataset:  18\n",
    "* Number of other people in this dataset:  128\n",
    "* Total feature values missing:  1352\n",
    "* Total feature values:  3190\n",
    "* The percentage compared to all values:  42.3824451411\n",
    "\n",
    "The dataset is very limited and contains about 42% of missing values. I couldn't remove much of outliers in statistical way by calculating the quartile. I plotted the features to find most extreme outliers and I found 1 that needed removal. The key \"TOTAL\" had the totals of all salaries and it skewed the data. I removed it by poping \"Total\" from the dict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**II. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up using \"SelectKBest\" for feature selection. I found these scores for my features: \n",
    "\n",
    "- salary score is:  18.575703268\n",
    "- bonus score is:  21.0600017075\n",
    "- long_term_incentive score is:  10.0724545294\n",
    "- deferred_income score is:  11.5955476597\n",
    "- loan_advances score is:  0.21705893034\n",
    "- expenses score is:  7.24273039654\n",
    "- total_payments score is:  4.2049708583\n",
    "- exercised_stock_options score is:  6.23420114051\n",
    "- restricted_stock score is:  2.10765594328\n",
    "- total_stock_value score is:  8.86672153711\n",
    "\n",
    "the scale is different for all of them, for example emails to poi could have about 300 emails, while salary can go up to 100 000 and so on. \n",
    "\n",
    "I created a new feature called \"ratio of poi_to and poi_from\". I wanted to see what is the ratio of sent and received emails from poi and maybe find some patterns in this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My features were labeled so I used these algorithms to test the accuracies: Naive bayes, decision trees and K nearest neigbor. I ended up using naive bayes (gaussianNB) algorithm because it had the best performance when I tested using tester.py. From my testing I got these results: \n",
    "- GaussianNb: Accuracy: 0.76800\tPrecision: 0.19544\tRecall: 0.16300\tF1: 0.17775\tF2: 0.16860\n",
    "- DecisionTree: Accuracy: 0.75677\tPrecision: 0.24428\tRecall: 0.27750\tF1: 0.25983\tF2: 0.27015\n",
    "- KNeigbors: Accuracy: 0.81231\tPrecision: 0.00673\tRecall: 0.00150\tF1: 0.00245\tF2: 0.00178\n",
    "\n",
    "I was actualy surprised that Kneigbors got really low scores, I was expecting it to be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters in my case means that I used different parameters in the classifier to get the best result and least overplotting. I tuned my parameters by using automatic parameter tuning with GridSearchCV and piping. GridSearchCV found the most optimal parameters to get the best result in my algorithm. \n",
    "\n",
    "If I wouldn't have used the tuning and used the default values, I would have got lower precission and recall scores. I had to use tuning to get the best result possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is a strategy to separate your data to train and test sets so the results would not be overfitted and the most accurate and not corrupted by the training. The classic mistake is to test on your training data which gives you bad results.\n",
    "\n",
    "I validated my data using StratifiedShuffleSplit and train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My selected evaluation metrics are precission and recall. \n",
    "\n",
    "- Precission: This is the ratio of predicted labels (POI) that were actualy the persons of interest.\n",
    "\n",
    "- Recall: This is the ratio of how many items were labeled as POI that were actual POI's. This means the higher this score is, the better the prediction.\n",
    "\n",
    "In my case using tester.py, I got precission of 0.39855 and recall of 0.33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
